# workload API

workload 리소스는 클러스터에 컨테이너를 가동시키기 위해 사용되는 리소스로
pod를 최소 단위로하여 pod를 관리하는 상위 리소스들이 있는 부모자식 관계로 구성되어있다

| 상위 | 중위 | 하위 |
| --- | --- | --- |
| - | replication controller | pod |
| deployment | replica set | pod |
| - | daemon set | pod |
| - | stateful set | pod |
| cronjob | job | pod |

## Pod

workload 리소스의 최소단위로, 하나 이상의 컨테이너로 구성된다
같은 pod내 컨테이너들은 같은 IP 주소를 갖기에 localhost로 서로 통신할 수 있다

pod 디자인에는 일반적으로 3가지 패턴이 있다

- sidecar pattern
    - pod 내부에 보조 역할(git, S3 업로드등)의 역할을 하는 컨테이너들을 추가하는 패턴

- ambassador pattern
    - 메인 컨테이너가 외부와 통신할 때 proxy역할을 하는 서브 컨테이너를 추가하는 패턴
    메인 컨테이너는 localhost로 proxy에만 데이터를 던져주면 될 뿐 통신에 관해 고민할 필요가 없다

- adapter pattern
    - 데이터 형식을 변환해주는 adapter 컨테이너를 추가하는 패턴으로 
    데이터 형식에 대한 고민 없이 localhost로 adapter 컨테이너로 데이터를 던져주면 되게 만드는 패턴

### command & args

쿠버네티스의 용어는 도커의 것과 조금 다르다
쿠버네티스에서 docker compose의 ENTRYPOINT는 *command*, CMD는 *args*라고 부른다 

**`ENTRYPOINT`**와 **`CMD`**는 컨테이너가 시작될 때 실행할 명령을 정의하는 데 사용하지만
**`ENTRYPOINT`**가 컨테이너의 실행 가능한 파일을 설정하고, 
**`CMD`**는 **`ENTRYPOINT`**에 전달될 기본 인자를 제공한다

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: sample-entrypoint
spec:
  containers:
  - name: nginx-container-112
    image: nginx:1.16
    command: ["/bin/sleep"] # 도커 이미지의 ENTRYPOINT 덮어쓰기 
    args: ["3600"] # 도커 이미지의 CMD 덮어쓰기
```

### hostNetwork

pod에 할당하는 IP주소는 외부에서 볼 수 없는 IP주소가 할당되지만 hostNetwork를 활성화하면 호스트 IP와 같은 구성으로 네트워크를 구성할 수 있다
하지만 포트번호 충돌을 방지하기  위해 일반적으로 사용하지 않는다

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: sample-hostnetwork
spec:
  hostNetwork: true # 호스트 IP와 같은 구성
  containers:
  - name: nginx-container
    image: nginx:1.16
```

### dnsPolicy

- ClusterFirst(기본값)
    - pod는 먼저 클러스터 내부 DNS를 이용하여 이름을 해석하고 해석이 안되는 경우 upstream DNS서버에 질의한다
- None
    - DNS서버를 수동으로 설정할경우 None으로 설정하고 dnsConfig에 설정하고 싶은 값을 작성한다
    - 정적으로 외부 DNS서버만 설정하면 클러스터 내부 DNS를 사용한 서비스 디스커버리는 사용할 수 없게된다
- Default
    - 쿠버네티스 node의 DNS 설정을 그대로 상속받는 경우이다
- ClusterFirstWithHostNet
    - hostNetwork 설정시 클러스터 내부의 DNS를 참조하고 싶은 경우 사용

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: sample-dnspolicy-clusterfirst
spec:
  dnsPolicy: ClusterFirst #dnsPolicy 설정하는 부분
  containers:
  - name: nginx-container
    image: nginx:1.16
```

### /etc/hosts

리눅스에서 DNS로 호스트명 해석하기 전 /etc/hosts 파일로 호스트명을 해석한다

쿠버네티스에서는 hostAliases를 변경하여 이를 변경할 수 있다

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: sample-hostaliases
spec:
  containers:
  - name: nginx-container
    image: nginx:1.16
  hostAliases: # /etc/hosts 설정
  - ip: 8.8.8.8
    hostnames:
    - google-dns
    - google-public-dns
```

### 컨테이너 내부 작업 디렉터리 변경

컨테이너 내부 작업 디렉터리는 Dockerfile의 WORKDIR로 설정하지만
쿠버네티스의 workingDir로 이를 덮어쓸 수 있다

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: sample-workingdir
spec:
  containers:
  - name: nginx-container
    image: nginx:1.16
    workingDir: /tmp # 작업 디렉터리 덮어쓰기
```

## replicaSet/replicaController

pod의 복제본(replica)을 생성하고 지정한 개수를 유지하는 리소스

### replicaSet 생성

replicaSet을 생성하는 manifest는 아래와 같으며
생성되는 pod명은 <replicaSet이름>-<임의의 문자열>로 생성된다

```yaml
apiVersion: apps/v1
kind: ReplicaSet # replicaSet 생성
metadata:
  name: sample-rs
spec:
  replicas: 3 # 유지할 복제본 개수
  selector:
    matchLabels:
      app: sample-app
  template:
    metadata:
      labels:
        app: sample-app
    spec:
      containers:
      - name: nginx-container
        image: nginx:1.16 # 사용할 이미지
```

### 자동 복구

node 또는 pod에서 문제가 생겼을 때, 다른 정상적인 node에서 pod를 생성하여 replicaSet 개수를 유지시킨다

파드 수 증감 이력은 `kubectl describe rs`로 확인할 수 있다

### replicaSet과 label

label을 다룰 때 이야기했듯, 유지할 복제본 개수는 특정 label을 가진 pod수를 계산하는 식으로 이뤄진다

```yaml
apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: sample-rs-fail
spec:
  replicas: 3
  selector:
    matchLabels: # 해당 이름을 갖는 label로 개수를 계산한다
      app: sample-app
  template:
    metadata:
      labels:
        app: sample-app-fail # label명이 다르다 selector와 이름이 일치하지 않아 에러
    spec:
      containers:
      - name: nginx-container
        image: nginx:1.16
```

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: sample-rs-pod
  labels:
		# replicaSet 외부에서 matchLabels와 일치하는 label을 설정할 경우 
		# replicaSet 개수에 포함되어 개수가 넘어갈 경우 기존의 replicaSet이 삭제될 수 있다
    app: sample-app 

spec:
  containers:
  - name: nginx-container
    image: nginx:1.17
```

### replicaSet과 scaling

복제본 개수를 변경하고 싶을 경우 두가지 방법이 있다

```yaml
# 복제본 개수를 3 -> 4로 변경한 manifest apply
sed -i -e 's|replicas: 3|replicas: 4|' sample-rs.yaml
kubectl apply -f sample-rs.yaml

# scale 명령어를 이용하여 변경
kubectl scale replicaset sample-rs --replicas 5

# replicaSet 목록 보기
kubectl get replicasets
```

## Deployment

여러 replicaSet을 관리하여 rolling update, rollback등을 구현하는 리소스

rolling update는 새로운 manifest가 적용된 신규 replicaSet 개수를 단계적으로 늘리고 
이에 비례해서 기존 replicaSet이 0이 될 때까지 단계적으로 줄이는 업데이트 방식이다

pod만으로 배포한 경우 장애 발생시 자동 재생성이 되지않고,
replicaSet으로만 배포한 경우 rolling update를 사용할 수 없다

```yaml
apiVersion: apps/v1
kind: Deployment # deployment 생성
metadata:
  name: sample-deployment
spec:
  replicas: 3
  selector:
    matchLabels:
      app: sample-app
  template:
    metadata:
      labels:
        app: sample-app
    spec:
      containers:
      - name: nginx-container
        image: nginx:1.16
```

### Deployment 업데이트 조건

spec.template의 변경이 있으면 업데이트를 실행한다. 이는 template 아래를 hash값으로 저장하여 해시값이 변했는지 체크함으로써 동작한다

### rollback

rollback의 동작은 사실 rolling update에서 replica 수가 0인 기존 replicaSet을 재사용하는 것이다

```yaml
# 1 버전으로 롤백한다 (버전을 설정하지 않으면 0(바로 이전 버전))
kubectl rollout undo deployment sample-deployment --to-revision 1 
```

### 업데이트 일시정지

```yaml
# 업데이트 일시정지
kubectl rollout pause deployment <deployment명>
# 업데이트 일시정지 해제
kubectl rollout resume deployment <deployment명>
```

### 일괄 업데이트

Recreate의 경우 모든 pod를 삭제하고 업데이트된 pod를 생성하여 모든 pod가 내려가므로 downtime이 존재하지만, 추가적인 리소스가 필요없고, 동시에 업데이트를 실행하기에 전환이 빠르다는 장점이 있다

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: sample-deployment-recreate
spec:
  strategy:
    type: Recreate # 모든 pod를 삭제하고 업데이트된 pod 생성
  replicas: 3
  selector:
    matchLabels:
      app: sample-app
  template:
    metadata:
      labels:
        app: sample-app
    spec:
      containers:
      - name: nginx-container
        image: nginx:1.16
```

### RollingUpdate

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: sample-deployment-rollingupdate
spec:
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxUnavailable: 0 # 동시에 정지 가능한 최대 pod 수
      maxSurge: 1 # 동시 생성 가능한 최대 pod 수
  replicas: 3
  selector:
    matchLabels:
      app: sample-app
  template:
    metadata:
      labels:
        app: sample-app
    spec:
      containers:
      - name: nginx-container
        image: nginx:1.16
```

## DaemonSet

replicaSet의 특수한 형태로, 각 노드에 pod를 하나씩 배치하는 리소스로, 
로깅과 모니터링용 pod를 배포할 때 유용하게 사용된다

### 업데이트 전략

Deployment와 동일한 RollingUpdate, 그리고 OnDelete가 있다
`RollingUpdate`는 pod가 재생성 되었을 때 새로운 정의로 pod를 생성하는 방식이고
`OnDelete`는 로깅과 모니터링에 주로 쓰이는 DaemonSet 특성상 기능을 멈추지않고 유지하기위해 manifest를 변경했더라도 기존 pod를 업데이트하지 않는 방식이다

### StatefulSet

데이터베이스, clustered 애플리케이션등에 적합한 replicaSet으로, 
파드명이 바뀌지 않으며 데이터를 영구적으로 저장하기 위한 구조로 되어있다

```yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: sample-statefulset
spec:
  serviceName: sample-statefulset
  replicas: 3
  selector:
    matchLabels:
      app: sample-app
  template:
    metadata:
      labels:
        app: sample-app
    spec:
      containers:
      - name: nginx-container
        image: nginx:1.16
        volumeMounts:
        - name: www
          mountPath: /usr/share/nginx/html\
	# 각 pod에 영구 볼륨 클레임을 지정할 수 있다
	# 이를 이용하면 클러스터 외부 네트워크를 이용해 제공되는 영구 볼륨에 pod를 연결할 수 있다
  volumeClaimTemplates: 
  - metadata:
      name: www
    spec:
      accessModes:
      - ReadWriteOnce
      resources:
        requests:
          storage: 1G
```

### statefulSet Scaling

scale out(증축)시 index가 가장 작은 pod부터 생성하고, 
scale in(감축)시 index가 가장 큰 pod부터 삭제한다
따라서 0번째 pod를 마스터 노드로 사용하는 이중화 구조 앱에 적합하다

### statefulSet Life cycle

podManagementPolicy 설정이 OrderedReady, Parallel인지에 따라 다르게 동작한다
OrderedReady의 경우 pod가 Ready 상태가 되어야 다음 pod를 실행시키고
Parallel의 경우 병렬로 동시에 pod를 가동시킨다

```yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: sample-statefulset-parallel # life cycle 설정
spec:
  podManagementPolicy: Parallel
  serviceName: sample-statefulset-parallel
  replicas: 3
  selector:
    matchLabels:
      app: sample-app
  template:
    metadata:
      labels:
        app: sample-app
    spec:
      containers:
      - name: nginx-container
        image: nginx:1.16
```

### 업데이트 전략

DaemonSet과 동일하다

### StatefulSet삭제와 영구 볼륨 삭제

영구 볼륨은 StatefulSet이 삭제되더라도 동시에 해제되지않는다.
반대로 영구 볼륨을 해제하지않고 StatefulSet을 생성한 경우 영구 볼륨 데이터는 그대로 pod에 기동된다

## Job

컨테이너를 사용하여 한 번만 실행되는 리소스로, 
병렬로 실행하면서 지정한 횟수의 성공을 보장하는 리소스.

replicaSet과의 가장 큰 차이점은 pod의 정지가 정상 종료인 작업인지 여부이다
일반적으로 replicaSet에서는 정상 종료 횟수를 셀 수 없기에 batch 작업의 경우 job을 이용하는게 좋다

### Job 생성

```yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: sample-job
spec:
  completions: 1 # 정상 종료한 pod 수
  parallelism: 1 # 동시에 실행할 pod 수
  backoffLimit: 10 # 실패 허용 횟수
  template:
    spec:
      containers:
      - name: tools-container
        image: amsy810/tools:v2.0
        command: ["sleep"]
        args: ["60"]
			# Never로 설정할 경우 pod 내부 프로세스가 정지되면 
			# 새로운 pod를 만들어 job을 계속 실행하려고 한다
	    # OnFailure로 설정할 경우 RESTARTS 카운트가 증가하고 사용했던 파드를 재사용한다
			# 이 때 영구 볼륨이나 hostPath를 사용하지 않는다면 데이터가 유실된다
			restartPolicy: Never
```

### completions, paralleism, backoffLimit로 작업 특징 설정

| completions | paralleism | backoffLimit | 특징 |
| --- | --- | --- | --- |
| 1 | 1 | 0 | 단 1회만 실행 task |
| M | N | P | N개를 병렬로 실행 task |
| - | 1 | P | 한 개씩 실행 queue |
| - | N | P | N개 병렬로 실행 queue |

## CronJob

정해진 시간에 Job을 생성하는 리소스

CronJob이 Job을 관리하고, Job이 pod를 관리하는 관계

### CronJob 생성

```yaml
apiVersion: batch/v1beta1
kind: CronJob
metadata:
  name: sample-cronjob
spec:
  schedule: "*/1 * * * *" # Job을 언제 실행시킬지 스케줄링
  concurrencyPolicy: Allow
  startingDeadlineSeconds: 30
  successfulJobsHistoryLimit: 5
  failedJobsHistoryLimit: 3
  suspend: false
  jobTemplate:
    spec:
      completions: 1
      parallelism: 1
      backoffLimit: 0
      template:
        spec:
          containers:
          - name: tools-container
            image: amsy810/random-exit:v2.0
          restartPolicy: Never
```

### CronJob 일시정지 / 임의 시점 실행

점검 등의 이유로 Job 생성을 원치 않을 경우 일시정지할 수 있고,
—from 옵션을 이용하여 정기적이지 않은 시점에서 CronJob을 생성할 수 있다

```yaml
# 일시 정지
kubectl patch cronjob <cronjob명> -p '{"spec":{"suspend":true}}'
# 임의 시점 실행
kubectl create job <임의 시점 CronJob명> --from cronjob/<CronJob명>
```

### 실행 시작 기간 제어

CronJob을 00시 00분으로 설정했지만 모종의 이유로 master가 종료되어 00시 00분에 CronJob을 실행하지 못하더라도 일정 시간 이후에도 실행시킬 수 있도록 지정할 수 있다
startingDeadlineSeconds=300으로 설정할 경우 00시 00분에 문제가 생겨 실행시키지 못하더라도, 
00시 05분 안에 정상으로 돌아온 경우 CronJob을 실행시킨다

### CronJob history

CronJob이 작업 후 Job을 몇 개까지 유지할 지 지정하는 설정.
`successfulJobsHistoryLimit`은 성공한 Job의 개수, `failedJobsHistoryLimit`은 실패한 Job의 개수

kubectl get jobs 명령어로 Job들을 확인할 수 있다
